{"cells":[{"metadata":{},"id":"medium-humidity","cell_type":"markdown","source":"## This notebook introduces the K Nearest Neighbor classification model\n\n### Background Info:\n#### KNN classification model is used to classify the category of a new observation based on past observations. It searches for K number of past (or known) observations that are the closest (in numerical euclidean distance) to the observation that needs to be classified, and based on the nearest values it determines the category of the new observation.\n\n### Steps KNN uses to predict classification of new observation:\n#### Step 1: Pick a value for K\n#### Step 2: Search for K observations in the training data that are the \"nearest\" to the measurements of the \n#### unknow observation\n#### Step 3: Use the most popular response value from the K nearest neighbors as the presicted reponse values for the unknown observation\n\n### Data requirements for working with Scikit-Learn:\n#### Requirement 1: Features and response/target must be separate objects\n#### Requirement 2: Features and response/target must be numeric data type\n#### Requirement 3: Features and response/target must be Numpy arrays\n#### Requirement 4: Features and response should have a specific and compatible shape (dimensions)\n\n### Steps to create and apply KNN model:\n#### Step 1: Import required libraries (such as pandas and numpy) and read/load dataset\n#### Step 2: Separate features from response/target by assigning feature matrix to variable 'X' and target vector to variable 'y'\n##### Note: convert to Numpy arrays if needed before assigning to 'X' & 'y'\n#### Step 3 (optional): Verify X & y have appropriate shapes\n#### Step 4: Import the class/model you plan to use\n#### Step 5: 'Instantiate' (i.e. make an instance of) the 'estimator' (i.e. scikit-learn's term for model)\n#### Step 6 (optional): Specify tuning parameters (aka 'hyperparameters') if required\n##### Note: hyperparameters refer to the model selection task, or algorithm hyperparameters, that in principle have no influence on the performance of the model but affect the speed and quality of the learning process.\n#### Step 7: Fit/train the model with data (aka \"model training\")\n#### Step 8: Use model to predict classification of new observation"},{"metadata":{"trusted":true},"id":"constant-original","cell_type":"code","source":"#Steps 1 & 2 - import reuqired libraries and load dataset\nimport pandas as pd\nimport numpy as np\n\n#import iris dataset\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\n#store feature matrix in \"X\" - here 'X' is in uppercase to denote that it represents a matrix (in this case a 150x4\n#matrix since it contains 4 features (i.e. columns/attributes) and 150 observations (i.e. rows/records))\nX = iris.data\n\n#store respone/target vector in 'y' - here 'y' is in lowercase to denote that it represent a vector (i.e. a \n#1-Dimensional series)\ny = iris.target","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"id":"exempt-bracelet","cell_type":"code","source":"#Step 3 (optional) - verify shape of X and y\nprint(X.shape)\nprint(y.shape)","execution_count":2,"outputs":[{"output_type":"stream","text":"(150, 4)\n(150,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"id":"three-billy","cell_type":"code","source":"#Step 4 - import KNN classifier model\nfrom sklearn.neighbors import KNeighborsClassifier","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"id":"adolescent-tooth","cell_type":"code","source":"#Step 5 & 6 - create instance of KNN classifier and specify hyperparameter - here we are selecting 1 as the value\n#for the hyper/tuning parameter 'K')\nknn = KNeighborsClassifier(n_neighbors=1) #note, KNeighborsClassifier function has additional parameters that are set\n#to their default scikit-Learn values. Use 'print knn to view the default parametrs'.","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"id":"delayed-decimal","cell_type":"code","source":"#Step 7 - train model (teach model relationship between feature and target/response)\nknn.fit(X,y)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"KNeighborsClassifier(n_neighbors=1)"},"metadata":{}}]},{"metadata":{"trusted":true},"id":"oriented-bracelet","cell_type":"code","source":"#Step 8 - predict classification of new onservation\ni = np.array([3,5,4,2])\nknn.predict(i.reshape(1,-1))#using reshape because inpt must be 2-Dimensional\n\n#you can also do prediction on multiple observations by passing in a list of lists as follows\ni = [[3,5,4,2],[5,4,3,2],[3,2,3,2]]\nknn.predict(i)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"array([2, 1, 1])"},"metadata":{}}]},{"metadata":{"trusted":true},"id":"palestinian-characterization","cell_type":"code","source":"#let's try using a different value for k (this is called model tuning in which you are varying the arguments passed to\n#the model)\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X,y)\nknn.predict(i)","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"array([1, 1, 1])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's try a different classification model such as Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X,y)\nlogreg.predict(i)","execution_count":15,"outputs":[{"output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","name":"stderr"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"array([0, 0, 0])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Conclusion: As can be seen from above example, differnet models generate different results. In the above example, we cannot tell which model is more accurate as we are using 'out of sample' data to predict (i.e. data that we do not know the true results for). In order to determine which model is a better fit, we need to evaluate the accuracy of each model using exising labeled data. This will also allow us to determine a better value for K when using knn.\n\nAdditional Resources:\n- www.scikit-learn.org\n- www.dataschool.io/15-hours-of-expert-machine-learning-videos/"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":5}